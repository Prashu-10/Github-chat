[Document(page_content='<div align="center">\n <h1> baichuan-7B </h1>\n<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">\n 🤗 \n <a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" style="margin-right: 15px; margin-left: 10px">Hugging Face</a> • \n 🤖\n <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank" style="margin-left: 10px">ModelScope</a > •\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/media/wechat.jpeg?raw=true" target="_blank" rel="noopener noreferrer" style="display: inline-block; margin-left: 10px">\n <span style="color: blue;">Wechat</span>\n </a>\n </p>\n\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)\n<h4 align="center">\n <p>\n <b>中文</b> |\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/README_EN.md">English</a>\n <p>\n</h4>\n\n\n</div>\n\n# 介绍\n\nbaichuan-7B 是由百川智能开发的一个开源可商用的大规模预训练语言模型。基于 Transformer 结构，在大约1.2万亿', metadata={}), Document(page_content='tokens 上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威 benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。\n\n## 数据\n\n* 原始数据包括开源的中英文数据和自行抓取的中文互联网数据，以及部分高质量知识性数据。\n* 参考相关数据工作，频率和质量是数据处理环节重点考虑的两个维度。 我们基于启发式规则和质量模型打分，对原始数据集进行篇章和句子粒度的过滤。在全量数据上，利用局部敏感哈希方法，对篇章和句子粒度做滤重。\n\n整体流程如下所示：\n<p align="center">\n <br>\n <img src="media/data_process.png" width="90%"/>\n <br>\n</p>\n\n* 经过不断的调整和多轮测试，最终确认了一个在下游任务上表现最好的中英文配比。\n* 我们使用了一个基于自动学习的数据权重策略，对不同类别的数据进行配比。\n\n## 分词\n我们参考学术界方案使用 SentencePiece 中的 byte pair encoding (BPE)作为分词算法，并且进行了以下的优化：\n1. 目前大部分开源模型主要基于英文优化，因此对中文语料存在效率较低的问题。我们使用2000万条以中英为主的多语言语料训练分词模型，显著提升对于中文的压缩率。\n2. 对于数学领域，我们参考了 LLaMA 和 Galactica 中的方案，对数字的每一位单独分开，避免出现数字不一致的问题，对于提升数学能力有重要帮助。\n3. 对于罕见字词（如特殊符号等），支持 UTF-8-characters 的 byte 编码，因此做到未知字词的全覆盖。 \n4. 我们分析了不同分词器对语料的压缩率，如下表，可见我们的分词器明显优于 LLaMA, Falcon 等开源模型，并且对比其他中文分词器在压缩率相当的情况下，训练和推理效率更高。\n\n| Model | baichuan-7B | LLaMA | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n|---------------|-------------|-------|--------|--------|---------|---------------|\n| Compress Rate | 0.737 | 1.312 | 1.049 | 1.206 | 0.631 |', metadata={}), Document(page_content='0.659 |\n| Vocab Size | 64000 | 32000 | 65024 | 50254 | 130344 | 106029 |\n\n## 模型结构\n整体模型基于标准的 Transformer 结构，我们采用了和 LLaMA 一样的模型设计\n* 位置编码：[rotary-embedding](https://arxiv.org/abs/2104.09864) 是现阶段被大多模型采用的位置编码方案，具有更好的外延效果。虽然训练过程中最大长度为4096，但是实际测试中模型可以很好的扩展到 5000 tokens 上，如下图：\n <p align="center">\n <br>\n <img src="media/long-context-ppl.png" width="90%"/>\n <br>\n </p>\n* 激活层：SwiGLU, Feedforward 变化为(8/3)倍的隐含层大小，即11008\n* Layer-Normalization: 基于 [RMSNorm](https://arxiv.org/abs/1910.07467) 的 Pre-Normalization\n\n## 训练稳定性和吞吐\n我们在原本的LLaMA框架上进行诸多修改以提升训练时的吞吐，具体包括：\n1. 算子优化技术：采用更高效算子，如 Flash-attention，NVIDIA apex 的 RMSNorm 等。 \n2. 算子切分技术：将部分计算算子进行切分，减小内存峰值。 \n3. 混合精度技术：降低在不损失模型精度的情况下加速计算过程。 \n4. 训练容灾技术：训练平台和训练框架联合优化，IaaS + PaaS 实现分钟级的故障定位和任务恢复。 \n5. 通信优化技术，具体包括： \n 1. 采用拓扑感知的集合通信算法，避免网络拥塞问题，提高通信效率。 \n 2. 根据卡数自适应设置 bucket size，提高带宽利用率。 \n 3. 根据模型和集群环境，调优通信原语的触发时机，从而将计算和通信重叠。\n\n基于上述的几个优化技术，我们在千卡A800机器上达到了7B模型182Tflops的吞吐，GPU峰值算力利用率高达58.3% 。\n \n\n最终的loss如下图：\n<p align="center">\n <br>\n <img src="media/7b.loss.png" width="90%"/>\n <br>\n</p>\n\n#', metadata={}), Document(page_content='公开benchmark榜单\n\n## 中文评测\n### C-Eval\n[C-Eval 数据集](https://cevalbenchmark.com/index.html)是一个全面的中文基础模型评测数据集，涵盖了52个学科和四个难度的级别。我们使用该数据集的dev集作为 few-shot 的来源，在 test 集上进行了 5-shot 测试。\n\n先修改 `evaluate_zh.py` 中的 OPENMODEL_PATH 和 CEVAL_DATA_PATH 两个值，分别是模型（文件夹）存放的路径和 C-Eval 数据集的路径。再执行下面的脚本。\n\n```shell\nshot=5 # few-shot\ngpu=0 # 显卡id\nsplit=test # 评估测试集\nmodel_id=baichuan-7b # 待评估的模型\ntask=ceval # 任务名称：ceval\necho gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot\nnohup python evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail > ${model_id}_${task}_${split}_${shot}-shot_record.txt 2>&1 &\n```\n\n### 结果\n\n| Model 5-shot | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n| Claude-v1.3 | 54.2 | 39.0 | 51.9 | 61.7 | 52.1 | 53.7 |\n|', metadata={}), Document(page_content='Claude-instant-v1.0 | 45.9 | 35.5 | 43.1 | 53.8 | 44.2 | 45.4 |\n| moss-moon-003-base (16B) | 27.4 | 24.5 | 27.0 | 29.1 | 27.2 | 26.9 |\n| Ziya-LLaMA-13B-pretrain | 30.2 | 22.7 | 27.7 | 34.4 | 32.0 | 28.9 |\n| LLaMA-7B-hf | 27.1 | 25.9 | 27.1 | 26.8 | 27.9 | 26.3 |\n| ChatGLM-6B | 34.5 | 23.1 | 30.4 | 39.6 | 37.4 | 34.5 |\n| Falcon-7B | 25.8 | 24.3 | 25.8 | 26.0 | 25.8 | 25.6 |\n| Open-LLaMA-v2-pretrain (7B) | 24.0 | 22.5 | 23.1 | 25.3 | 25.2 | 23.2 |\n| TigerBot-7B-base | 25.7 | 27.0 | 27.3 | 24.7 | 23.4 | 26.1 |\n| Aquila-7B<sup>*</sup> | 25.5 | 25.2 | 25.6 | 24.6 | 25.2 | 26.6 |\n| BLOOM-7B | 22.8 | 20.2 | 21.8 | 23.3 | 23.9 | 23.3 |\n| BLOOMZ-7B | 35.7 | 25.8 | 31.3 | 43.5 | 36.6 | 35.6 |\n| **baichuan-7B** | 42.8 | 31.5 | 38.2 | 52.0 | 46.2 | 39.3 |\n\n\n### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) 是一个以中国高考题作为评测大语言模型能力的数据集，用以评估模型的语言能力和逻辑推理能力。\n我们只保留了其中的单项选择题，随机划分后对所有模型进行统一 5-shot 测试。\n\n### 结果\n以下是测试的结果。\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 21.41 |\n|', metadata={}), Document(page_content='Ziya-LLaMA-13B-pretrain | 23.17 |\n| Falcon-7B | 23.98 |\n| TigerBot-7B-base | 25.94 |\n| LLaMA-7B | 27.81 |\n| ChatGLM-6B | 21.41 |\n| BLOOM-7B | 26.96 |\n| BLOOMZ-7B | 28.72 |\n| Aquila-7B<sup>*</sup> | 24.39 |\n| **baichuan-7B** | **36.24** |\n\n\n### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) 旨在评估模型的认知和解决问题相关的任务中的一般能力。\n我们只保留了其中的四选一单项选择题，随机划分后对所有模型进行了统一5-shot测试。\n\n### 结果\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 23.49 |\n| Ziya-LLaMA-13B-pretrain | 27.64 |\n| Falcon-7B | 27.18 |\n| TigerBot-7B-base | 25.19 |\n| LLaMA-7B | 28.17 |\n| ChatGLM-6B | 23.49 |\n| BLOOM-7B | 26.55 |\n| BLOOMZ-7B | 30.27 |\n| Aquila-7B<sup>*</sup> | 25.58 |\n| **baichuan-7B** | **34.44** |\n\n<sup>*</sup>其中 Aquila 模型来源于智源官方网站(https://model.baai.ac.cn/model-detail/100098) 仅做参考\n\n## 英文榜单\n除了中文之外，我们也测试了模型在英文上的效果，[MMLU](https://arxiv.org/abs/2009.03300) 是包含57个多选任务的英文评测数据集，涵盖了初等数学、美国历史、计算机科学、法律等，难度覆盖高中水平到专家水平，是目前主流的LLM评测数据集。\n\n我们采用了[开源](https://github.com/hendrycks/test) 的评测方案，最终 5-shot', metadata={}), Document(page_content='结果如下所示：\n\n### 结果\n\n| Model | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n| LLaMA-7B<sup>2</sup> | 34.0 | 38.3 | 30.5 | 38.1 | 35.1 |\n| Falcon-7B<sup>1</sup> | - | - | - | - | 35.0 |\n| mpt-7B<sup>1</sup> | - | - | - | - | 35.6 |\n| ChatGLM-6B<sup>0</sup> | 35.4 | 41.0 | 31.3 | 40.5 | 36.9 |\n| BLOOM-7B<sup>0</sup> | 25.0 | 24.4 | 26.5 | 26.4 | 25.5 |\n| BLOOMZ-7B<sup>0</sup> | 31.3 | 42.1 | 34.4 | 39.0 | 36.1 |\n| moss-moon-003-base (16B)<sup>0</sup> | 24.2 | 22.8 | 22.4 | 24.4 | 23.6 |\n| moss-moon-003-sft (16B)<sup>0</sup> | 30.5 | 33.8 | 29.3 | 34.4 | 31.9 |\n| **baichuan-7B<sup>0</sup>** | **38.4** | **48.9** | **35.6** | **48.1** | **42.3** |\n\n### 上标说明：\n\n 0:重新复现\n 1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n 2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n\n### 复现方法\n```shell\ngit clone https://github.com/hendrycks/test\ncd test\nwget', metadata={}), Document(page_content='https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data\nmkdir results\ncp evaluate_mmlu.py .\npython evaluation/evaluate_mmlu.py -m /path/to/baichuan-7b\n\n```\n\n其中在 MMLU 上57个任务的具体细指标如下图：\n<p align="center">\n <br>\n <img src="media/MMLU-57-tasks.png" width="90%"/>\n <br>\n</p>\n\n其中各个学科的指标如下图：\n<p align="center">\n <br>\n <img src="media/MMLU 21 Subjects.png" width="90%"/>\n <br>\n</p>\n\n# 推理方法\n\n推理代码已经在[官方 Huggingface 库](https://huggingface.co/baichuan-inc/baichuan-7B) \n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(\'登鹳雀楼->王之涣\\n夜雨寄北->\', return_tensors=\'pt\')\ninputs = inputs.to(\'cuda:0\')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n# 训练方法\n## 安装依赖\n```shell\npip install -r', metadata={}), Document(page_content='requirements.txt\n```\n## 准备数据\n用户将训练语料按总rank数的倍数均匀切分成多个 UTF-8 文本文件，放置在语料目录（默认为 `data_dir` ）下。各个rank进程将会读取语料目录下的不同文件，全部加载到内存后，开始后续训练过程。以上是简化的示范流程，建议用户在正式训练任务中，根据需求调整数据生产逻辑。\n\n## 下载 tokenizer 模型\n下载 tokenizer 模型文件 [tokenizer.model](https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model) ，放置在项目目录下。\n \n## 配置 DeepSpeed\n本示范代码采用 DeepSpeed 框架进行训练。用户需根据集群情况，修改 `config/hostfile` ，如果是多机多卡，需要修改 ssh 中各个节点的 IP 配置。具体可以参见DeepSpeed[官方说明](https://www.deepspeed.ai/) 。\n\n## 执行训练\n```python\nscripts/train.sh\n```\n\n# 协议\n对本仓库源码的使用遵循开源许可协议 [Apache 2.0](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)。\n\nbaichuan-7B支持商用。如果将baichuan-7B 模型或其衍生品用作商业用途，请您按照如下方式联系许可方，以进行登记并向许可方申请书面授权：联系邮箱：opensource@baichuan-inc.com， 具体许可协议可见[《baichuan-7B 模型许可协议》](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)。', metadata={}), Document(page_content='<div align="center">\n <h1> baichuan-7B </h1>\n<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">\n 🤗 \n <a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" style="margin-right: 15px; margin-left: 10px">Hugging Face</a> • \n 🤖\n <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank" style="margin-left: 10px">ModelScope</a > •\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/media/wechat.jpeg?raw=true" target="_blank" rel="noopener noreferrer" style="display: inline-block; margin-left: 10px">\n <span style="color: blue;">Wechat</span>\n </a>\n </p>\n\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)\n<h4 align="center">\n <p>\n <b>English</b> |\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/README.md">中文</a>\n <p>\n</h4>\n\n\n</div>\n\n# Introduction\n\nbaichuan-7B is an open-source, large-scale pre-training', metadata={}), Document(page_content='language model developed by Baichuan Intelligent Technology. baichuan-7B is based on Transformer architecture, which contains 7 billion parameter and trained on approximately 1.2 trillion tokens. It supports both Chinese and English languages with a context window length of 4096. It has achieved the best performance among models of the same size on standard Chinese and English authoritative benchmarks (C-EVAL, MMLU, etc).\n\n## Data\n\n* The original corpora includes open-source Chinese and English data, self-crawled Chinese internet data, and some high-quality knowledge-intensive data.\n* Referring to related data work, frequency and quality are two dimensions that are considered important in the data processing stage. We apply heuristic rules and quality model scoring to filter the original dataset at both the paragraph and sentence levels. Employing the Locality-Sensitive Hashing (LSH) method on the full dataset, we perform de-duplication at both the paragraph and sentence levels.\n\nThe whole data processing', metadata={}), Document(page_content='process is shown below:\n<p align="center">\n <br>\n <img src="media/data_process.png" width="90%"/>\n <br>\n</p>\n\n* After continuous adjustments and multiple rounds of testing, we finally determined the best Chinese to English ratio that are optimized on downstream tasks.\n* We used an automatic algorithm-based data sampling strategy to balance the weights of different data categories.\n\n## Tokenization\nWe use the byte pair encoding (BPE) from SentencePiece as the tokenization algorithm, along with the following optimizations:\n\n1. Most open-source models are primarily optimized for English, resulting in low efficiency for Chinese corpus. So we trained the tokenizer using 20 million multilingual corpora mainly composed of Chinese and English, significantly improving the compression rate for Chinese.\n2. To improve the ability for mathmatics, we split all numbers into individual digits that is also adopted in LLaMA and Galactica, separately tokenizing each digit to avoid inconsistencies in numbers.\n3. For rare words', metadata={}), Document(page_content='(such as emoji and special symbols), we fallback unknown characters to byte encoding of UTF-8, thus achieving full coverage of unknown words.\n4. We analyzed the compression rate of different tokenizers on the corpus. As shown in the following table, our tokenizer significantly outperforms open-source models like LLaMA, Falcon, and others. Compared to other Chinese tokenizers with similar compression rates, it offers higher training and inference efficiency. \n\n| Model | baichuan-7B | LLaMA | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n|---------------|-------------|-------|--------|--------|---------|---------------|\n| Compress Rate | 0.737 | 1.312 | 1.049 | 1.206 | 0.631 | 0.659 |\n| Vocab Size | 64000 | 32000 | 65024 | 50254 | 130344 | 106029 |\n\n## Model Architecture\nThe overall model is based on the standard Transformer structure, and we have adopted a model design similar to that of LLaMA.\n* Positional Embeddings: [rotary-embedding](https://arxiv.org/abs/2104.09864) is the widely used positional encoding', metadata={}), Document(page_content='method, with better extrapolation effects. Although the maximum length during training is 4096, the model can be well extrapolated to 5000 tokens in inference time, as shown in the following diagram:\n <p align="center">\n <br>\n <img src="media/long-context-ppl.png" width="90%"/>\n <br>\n </p>\n* Activation：SwiGLU, and the dimension of the feedforward-layer is set to 11008\n* Layer-Normalization: We use the Pre-Normalization method based on [RMSNorm](https://arxiv.org/abs/1910.07467)\n\n## Training stability and Throughput\nWe made numerous modifications to the original LLaMA framework to improve throughput during training, including:\n\n1. Operator optimization technology: We adopted more efficient operators, such as Flash-attention, NVIDIA apex\'s RMSNorm, etc.\n2. Tensor partitioning technology: We partitioned some computational operators to reduce peak memory usage.\n3. Mixed-precision technology: This accelerates the computational process without sacrificing model accuracy.\n4. Training failure recovery technology: The', metadata={}), Document(page_content='training platform and the training framework were jointly optimized. By combining IaaS and PaaS, we can locate faults and recover tasks within minutes.\n5. Communication optimization technology which includes:\n 1. Topology-aware collective communication algorithms to avoid network congestion and improve communication efficiency.\n 2. Adaptive setting of bucket size based on the number of cards to improve bandwidth utilization.\n 3. Tuning the trigger timing of communication primitives based on the model and the cluster environment, thereby overlapping computation and communication.\n \nBy using these optimization techniques, we achieved a throughput of 182 Tflops for the 7B model on thousand A800 GPUs, with a peak GPU computing power utilization rate of up to 58.3%.\n\nThe final loss of the model is shown below：\n<p align="center">\n <br>\n <img src="media/7b.loss.png" width="90%"/>\n <br>\n</p>\n\n# Benchmark\n\n## Chinese Benchmarks\n### C-Eval\n[C-Eval](https://cevalbenchmark.com/index.html) is a comprehensive Chinese', metadata={}), Document(page_content='language models evaluation dataset, covering 52 subjects and four levels of difficulty. We used the dev set from this dataset as the source for few-shot learning and conducted a 5-shot test on the test set.\n\n\nChange OPENMODEL_PATH and CEVAL_DATA_PATH in evaluate_zh.py, corresponding to model directory and C-Eval dataset, and runing:\n```shell\nshot=5 # few-shot\ngpu=0 # GPUid\nsplit=test # test set\nmodel_id=baichuan-7b # model\ntask=ceval # task name：ceval\necho gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot\nnohup python evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail > ${model_id}_${task}_${split}_${shot}-shot_record.txt 2>&1 &\n\n```\n\n### Result\n\n\n| Model 5-shot | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n|', metadata={}), Document(page_content='Claude-v1.3 | 54.2 | 39.0 | 51.9 | 61.7 | 52.1 | 53.7 |\n| Claude-instant-v1.0 | 45.9 | 35.5 | 43.1 | 53.8 | 44.2 | 45.4 |\n| moss-moon-003-base (16B) | 27.4 | 24.5 | 27.0 | 29.1 | 27.2 | 26.9 |\n| Ziya-LLaMA-13B-pretrain | 30.2 | 22.7 | 27.7 | 34.4 | 32.0 | 28.9 |\n| LLaMA-7B-hf | 27.1 | 25.9 | 27.1 | 26.8 | 27.9 | 26.3 |\n| ChatGLM-6B | 34.5 | 23.1 | 30.4 | 39.6 | 37.4 | 34.5 |\n| Falcon-7B | 25.8 | 24.3 | 25.8 | 26.0 | 25.8 | 25.6 |\n| Open-LLaMA-v2-pretrain (7B) | 24.0 | 22.5 | 23.1 | 25.3 | 25.2 | 23.2 |\n| TigerBot-7B-base | 25.7 | 27.0 | 27.3 | 24.7 | 23.4 | 26.1 |\n| Aquila-7B<sup>*</sup> | 25.5 | 25.2 | 25.6 | 24.6 | 25.2 | 26.6 |\n| BLOOM-7B | 22.8 | 20.2 | 21.8 | 23.3 | 23.9 | 23.3 |\n| BLOOMZ-7B | 35.7 | 25.8 | 31.3 | 43.5 | 36.6 | 35.6 |\n| **baichuan-7B** | 42.8 | 31.5 | 38.2 | 52.0 | 46.2 | 39.3 |\n\n\n### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) is an evaluation dataset used in Chinese college entrance examination questions to evaluate the capabilities of large language models, assessing the', metadata={}), Document(page_content="model's language ability and logical reasoning skills. We processed the dataset to only containing the single-answer multiple choice questions, we conducted a 5-shot test on all models.\n\n### Results\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 21.41 |\n| Ziya-LLaMA-13B-pretrain | 23.17 |\n| Falcon-7B | 23.98 |\n| TigerBot-7B-base | 25.94 |\n| LLaMA-7B | 27.81 |\n| ChatGLM-6B | 21.41 |\n| BLOOM-7B | 26.96 |\n| BLOOMZ-7B | 28.72 |\n| Aquila-7B<sup>*</sup> | 24.39 |\n| **baichuan-7B** | **36.24** |\n\n\n### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) is a dataset aimed at evaluating the model's general abilities in cognitive and problem-solving tasks.\nwe conducted a 5-shot test on all models.\n\n### Result\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 23.49 |\n| Ziya-LLaMA-13B-pretrain | 27.64 |\n| Falcon-7B | 27.18 |\n| TigerBot-7B-base | 25.19 |\n| LLaMA-7B | 28.17 |\n| ChatGLM-6B | 23.49 |\n| BLOOM-7B | 26.55 |\n|", metadata={}), Document(page_content='BLOOMZ-7B | 30.27 |\n| Aquila-7B<sup>*</sup> | 25.58 |\n| **baichuan-7B** | **34.44** |\n\n<sup>*</sup>The Aquila-7b are not implemented on Huggingface yet so we derived the model from (https://model.baai.ac.cn/model-detail/100098), which may have not identical to their official result.\n\n## English Benchmarks\nIn addition to Chinese, we also tested the performance of the model in English. [MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty spans from high school level to expert level, making it a mainstream evaluation dataset for Large Language Models (LLMs).\n\nWe adopt the public implementation of (https://github.com/hendrycks/test) and the final result is shwon below：\n\n### Results on MMLU\n\n| Model | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n|', metadata={}), Document(page_content='LLaMA-7B<sup>2</sup> | 34.0 | 38.3 | 30.5 | 38.1 | 35.1 |\n| Falcon-7B<sup>1</sup> | - | - | - | - | 35.0 |\n| mpt-7B<sup>1</sup> | - | - | - | - | 35.6 |\n| ChatGLM-6B<sup>0</sup> | 35.4 | 41.0 | 31.3 | 40.5 | 36.9 |\n| BLOOM 7B<sup>0</sup> | 25.0 | 24.4 | 26.5 | 26.4 | 25.5 |\n| BLOOMZ 7B<sup>0</sup> | 31.3 | 42.1 | 34.4 | 39.0 | 36.1 |\n| moss-moon-003-base (16B)<sup>0</sup> | 24.2 | 22.8 | 22.4 | 24.4 | 23.6 |\n| moss-moon-003-sft (16B)<sup>0</sup> | 30.5 | 33.8 | 29.3 | 34.4 | 31.9 |\n| **baichuan-7B<sup>0</sup>** | **38.4** | **48.9** | **35.6** | **48.1** | **42.3** |\n\n### Notes：\n\n 0:Our implementation\n 1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n 2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n\n### How to implement by yourself\n```shell\ngit clone https://github.com/hendrycks/test\ncd test\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data\nmkdir results\ncp evaluate_mmlu.py .\npython evaluation/evaluate_mmlu.py -m', metadata={}), Document(page_content='/path/to/baichuan-7b\n\n```\n\nSpecifically, the result of 57 MMLU tasks is：\n<p align="center">\n <br>\n <img src="media/MMLU-57-tasks.png" width="90%"/>\n <br>\n</p>\n\nAnd the comparison of 21 different subjects is：\n<p align="center">\n <br>\n <img src="media/MMLU 21 Subjects.png" width="90%"/>\n <br>\n</p>\n\n# Inference\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(\'Hamlet->Shakespeare\\nOne Hundred Years of Solitude->\', return_tensors=\'pt\')\ninputs = inputs.to(\'cuda:0\')\npred = model.generate(**inputs, max_new_tokens=64)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n\n# Training\n## Install requirements\n```shell\npip install -r requirements.txt\n```\n## Prepare pre-training datasets\nYou should divide the training corpus into multiple UTF-8 text files', metadata={}), Document(page_content='evenly according to the multiple of the total rank number, and place them in the corpus directory (default is `data_dir`). Each rank processor will read different files in the corpus directory, load them all into memory, and then start the subsequent training process. The above is a simplified demonstration process. It is recommended that users adjust the data production logic according to their needs in formal training tasks.\n\n## Download tokenizer\nYou can download our [tokenizer.model](https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model) from the Huggingface, and place them in the root director.\n \n## Config DeepSpeed\nThis demo code uses the DeepSpeed framework for training. Users should modify `config/hostfile` according to the cluster conditions.\n\n## Start training\n```shell\nscripts/train.sh\n```\n\n\n# Licences\nThe use of the source code in this repository is governed by the open source license [Apache 2.0](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE) .\n\nThe use of the', metadata={}), Document(page_content='baichuan-7B model weights, however, must follow the [《baichuan-7B 模型许可协议》](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) .', metadata={})]