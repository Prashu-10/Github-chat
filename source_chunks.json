[Document(page_content='<!-- # ![icon](images/figs/icon.png) DreamSim Perceptual Metric -->\n<!-- # DreamSim Perceptual Metric <img src="images/figs/icon.png" align="left" width="50px"/> -->\n# DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data\n### [Project Page](https://dreamsim-nights.github.io/) | [Paper](https://arxiv.org/abs/2306.09344) | [Bibtex](#bibtex)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1taEOMzFE9g81D9AwH27Uhy2U82tQGAVI?usp=sharing)\n\n[Stephanie Fu](https://stephanie-fu.github.io)\\* $^{1}$, [Netanel Tamir](https://netanel-tamir.github.io)\\* $^{2}$, [Shobhita Sundaram](https://ssundaram21.github.io)\\* $^{1}$, [Lucy Chai](https://people.csail.mit.edu/lrchai/) $^1$, [Richard Zhang](http://richzhang.github.io) $^3$, [Tali Dekel](https://www.weizmann.ac.il/math/dekel/) $^2$, [Phillip Isola](https://web.mit.edu/phillipi/) $^1$. (*equal contribution)<br>\n$^1$ MIT, $^2$ Weizmann Institute of Science, $^3$ Adobe', metadata={}), Document(page_content='Research.\n\n![teaser](images/figs/teaser.png)\n\n**Summary**\n\nCurrent metrics for perceptual image similarity operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level differences in layout, pose, semantic content, etc. Models that use image-level embeddings such as DINO and CLIP capture high-level and semantic judgements, but may not be aligned with human perception of more finegrained attributes.\n\nDreamSim is a new metric for perceptual image similarity that bridges the gap between "low-level" metrics (e.g. LPIPS, PSNR, SSIM) and "high-level" measures (e.g. CLIP). Our model was trained by concatenating CLIP, OpenCLIP, and DINO embeddings, and then finetuning on human perceptual judgements. We gathered these judgements on a dataset of ~20k image triplets, generated by diffusion models. Our model achieves better alignment with human similarity judgements than existing metrics, and can be used for downstream applications', metadata={}), Document(page_content='such as image retrieval.\n\n## Requirements\n- Linux\n- Python 3\n- NVIDIA GPU + CUDA CuDNN (DreamSim is only supported on CUDA devices)\n\n## Setup\n\n**Option 1:** Install using pip: \n\n```pip install dreamsim```\n\nThe package is used for importing and using the DreamSim model.\n\n**Option 2:** Clone our repo and install dependencies.\nThis is necessary for running our training/evaluation scripts.\n\n```\npython3 -m venv ds\nsource ds/bin/activate\npip install -r requirements.txt\nexport PYTHONPATH="$PYTHONPATH:$(realpath ./dreamsim)"\n```\nTo install with conda:\n```\nconda create -n ds\nconda activate ds\nconda install pip # verify with the `which pip` command\npip install -r requirements.txt\nexport PYTHONPATH="$PYTHONPATH:$(realpath ./dreamsim)"\n```\n\n## Usage\n**For walk-through examples of the below use-cases, check out our [Colab demo](https://colab.research.google.com/drive/1taEOMzFE9g81D9AwH27Uhy2U82tQGAVI?usp=sharing).**\n\n### Quickstart: Perceptual similarity metric\nThe basic use case is to measure the perceptual distance', metadata={}), Document(page_content='between two images. **A higher score means more different, lower means more similar**. \n\nThe following code snippet is all you need. The first time that you run `dreamsim` it will automatically download the model weights. The default model settings are specified in `./dreamsim/config.py`.\n```\nfrom dreamsim import dreamsim\nfrom PIL import Image\n\nmodel, preprocess = dreamsim(pretrained=True)\n\nimg1 = preprocess(Image.open("img1_path")).to("cuda")\nimg2 = preprocess(Image.open("img2_path")).to("cuda")\ndistance = model(img1, img2) # The model takes an RGB image from [0, 1], size 1x3x224x224\n```\n\nTo run on example images, run `demo.py`. The script should produce distances (0.424, 0.34). \n\n### Feature extraction\nTo extract a *single image embedding* using dreamsim, use the `embed` method as shown in the following snippet:\n```\nimg1 = preprocess(Image.open("img1_path")).to("cuda")\nembedding = model.embed(img1)\n```\nThe perceptual distance between two images is the cosine distance between their embeddings. If the', metadata={}), Document(page_content='embeddings are normalized (true by default) L2 distance can also be used.\n\n\n### Image retrieval\nOur model can be used for image retrieval, and plugged into existing such pipelines. The code below ranks a dataset of images based on their similarity to a given query image. \n\nTo speed things up, instead of directly calling `model(query, image)` for each pair, we use the `model.embed(image)` method to pre-compute single-image embeddings, and then take the cosine distance between embedding pairs.\n```\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch.nn.functional as F\n\n# let query be a sample image.\n# let images be a list of images we are searching.\n\n# Compute the query image embedding\nquery_embed = model.embed(preprocess(query).to("cuda"))\ndists = {}\n\n# Compute the (cosine) distance between the query and each search image\nfor i, im in tqdm(enumerate(images), total=len(images)):\n img_embed = model.embed(preprocess(im).to("cuda"))\n dists[i] = (1 - F.cosine_similarity(query_embed, img_embed, dim=-1)).item()\n\n#', metadata={}), Document(page_content='Return results sorted by distance\ndf = pd.DataFrame({"ids": list(dists.keys()), "dists": list(dists.values())})\nreturn df.sort_values(by="dists")\n```\n\n### Perceptual loss function\nOur model can be used as a loss function for iterative optimization (similarly to the LPIPS metric). These are the key lines; for the full example, refer to the [Colab](https://colab.research.google.com/drive/1taEOMzFE9g81D9AwH27Uhy2U82tQGAVI?usp=sharing).\n```\nfor i in range(n_iters):\n dist = model(predicted_image, reference_image)\n dist.backward()\n optimizer.step()\n```\n<!--Experiments-->\n## NIGHTS (Novel Image Generations with Human-Tested Similarities) Dataset\nDreamSim is trained by fine-tuning on the NIGHTS dataset. For details on the dataset structure and creation, refer to the [dataset page](https://github.com/ssundaram21/dreamsim/tree/main/dataset).\n\nRun `./dataset/download_dataset.sh` to download and unzip the NIGHTS dataset into `./dataset/nights`. The unzipped dataset size is 58 GB. \n\n## Experiments\n\n### Download', metadata={}), Document(page_content='resources\nRun `./training/download_models.sh` to download and unzip necessary ViT checkpoints (for CLIP, OpenCLIP, and MAE) into `./models`. \n\n### Training\nTo finetune a perceptual model on the dataset, run `./training/train.py`. For example, to finetune an ensemble of DINO, CLIP, and OpenCLIP using LoRA, run:\n\n```\npython ./training/train.py --config ./configs/train_ensemble_model_lora.yaml\n```\n\nWe provide more sample configs in `./configs`, including examples of finetuning with LoRA and with an MLP head. See `./training/train.py` for a full list and description of training options.\n\n### Evaluation\nTo evaluate a perceptual model on the dataset, run `./training/evaluate.py`. For example, to evaluate dreamsim on the dataset, run:\n\n```\npython ./training/evaluate.py --config ./configs/eval_baseline.yaml\n```\n\nFor an example of evaluating using a trained checkpoint, refer to `./configs/eval_checkpoint.yaml`. See `./training/evaluate.py` for a full list and description of evaluation options.\n<!--Experiments-->\n\n<a', metadata={}), Document(page_content='name="bibtex"></a>\n## Citation\n\nIf you find our work or any of our materials useful, please cite our paper:\n```\n@misc{fu2023dreamsim,\n title={DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data}, \n author={Stephanie Fu and Netanel Tamir and Shobhita Sundaram and Lucy Chai and Richard Zhang and Tali Dekel and Phillip Isola},\n year={2023},\n eprint={2306.09344},\n archivePrefix={arXiv},\n primaryClass={cs.CV}\n}\n```\n\n## Acknowledgements\nOur code borrows from the ["Deep ViT Features as Dense Visual Descriptors"](https://dino-vit-features.github.io/) repository for ViT feature extraction, and takes inspiration from the [UniverSeg](https://github.com/JJGO/UniverSeg) respository for code structure.', metadata={}), Document(page_content='# NIGHTS perceptual dataset\n\nNIGHTS (Novel Image Generations with Human-Tested Similarities) is a dataset of 20,019 image triplets with human scores of perceptual similarity. Each triplet consists of a reference image and two distortions, located at `ref/xxx/yyy.png`, `distort/xxx/yyy_0.png`, and `distort/xxx/yyy_1.png`. \n\nDirectories are numbered `000` to `099` and files are numbered `000` to `999`, though there is not an image triplet at every number.\n\n## Dataset structure:\n\n```\nnights/\n├── ref/\n│ ├── 000/\n│ │ ├── 000.png\n│ │ ├── 001.png\n│ │ ├── ...\n│ │ └── 999.png\n│ └── 001/ 002/ ... 099/\n├── distort/\n│ ├── 000/\n│ │ ├── 000_0.png\n│ │ ├── 000_1.png\n│ │ ├── ...\n│ │ ├── 999_0.png\n│ │ └── 999_1.png\n│ └── 001/ 002/ ... 099/\n├── data.csv\n└── README.md\n```\n\n## Dataset generation details\nAll data was generated by Stable Diffusion 2.1 [1] by sampling image triplets with a prompt of the same category and different random seed, using the structure:\n`An image of a <category>`. The `category` is drawn from image labels', metadata={}), Document(page_content='in popular datasets - ImageNet [2], CIFAR-10 [3], CIFAR-100 [3], Oxford 102 Flower [4], Food-101 [5], and SUN397 [6].\n\nSee `data.csv` for the full list of categories corresponding to each image triplet.\n\n## Limitations\nWe note that by using Stable Diffusion, our benchmark is exposed to potential biases preexisting and sensitive content in the model. As such, we generate our images with a pre-defined set of categories, while largely avoiding human faces. Our perceptual model is also finetuned from existing pre-trained backbones, and thus may also inherit prior errors and biases.\n\n## References\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\nresolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition_, pages 10684–10695, 2022.\n\n[2] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In _2009 IEEE conference on', metadata={}), Document(page_content='computer vision and pattern\nrecognition_, pages 248–255. Ieee, 2009\n\n[3] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n\n[4] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large\nnumber of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image\nProcessing_, pages 722–729. IEEE, 2008.\n\n[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. _Food-101–mining discriminative\ncomponents with random forests. In Computer Vision–ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446–461_. Springer, 2014.\n\n[6] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In 2*010 IEEE computer society conference\non computer vision and pattern recognition*, pages 3485–3492. IEEE, 2010.', metadata={})]