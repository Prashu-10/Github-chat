[Document(page_content='<div align="center">\n <h1> baichuan-7B </h1>\n<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">\n ğŸ¤— \n <a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" style="margin-right: 15px; margin-left: 10px">Hugging Face</a> â€¢ \n ğŸ¤–\n <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank" style="margin-left: 10px">ModelScope</a > â€¢\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/media/wechat.jpeg?raw=true" target="_blank" rel="noopener noreferrer" style="display: inline-block; margin-left: 10px">\n <span style="color: blue;">Wechat</span>\n </a>\n </p>\n\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)\n<h4 align="center">\n <p>\n <b>ä¸­æ–‡</b> |\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/README_EN.md">English</a>\n <p>\n</h4>\n\n\n</div>\n\n# ä»‹ç»\n\nbaichuan-7B æ˜¯ç”±ç™¾å·æ™ºèƒ½å¼€å‘çš„ä¸€ä¸ªå¼€æºå¯å•†ç”¨çš„å¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚åŸºäº Transformer ç»“æ„ï¼Œåœ¨å¤§çº¦1.2ä¸‡äº¿', metadata={}), Document(page_content='tokens ä¸Šè®­ç»ƒçš„70äº¿å‚æ•°æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±åŒè¯­ï¼Œä¸Šä¸‹æ–‡çª—å£é•¿åº¦ä¸º4096ã€‚åœ¨æ ‡å‡†çš„ä¸­æ–‡å’Œè‹±æ–‡æƒå¨ benchmarkï¼ˆC-EVAL/MMLUï¼‰ä¸Šå‡å–å¾—åŒå°ºå¯¸æœ€å¥½çš„æ•ˆæœã€‚\n\n## æ•°æ®\n\n* åŸå§‹æ•°æ®åŒ…æ‹¬å¼€æºçš„ä¸­è‹±æ–‡æ•°æ®å’Œè‡ªè¡ŒæŠ“å–çš„ä¸­æ–‡äº’è”ç½‘æ•°æ®ï¼Œä»¥åŠéƒ¨åˆ†é«˜è´¨é‡çŸ¥è¯†æ€§æ•°æ®ã€‚\n* å‚è€ƒç›¸å…³æ•°æ®å·¥ä½œï¼Œé¢‘ç‡å’Œè´¨é‡æ˜¯æ•°æ®å¤„ç†ç¯èŠ‚é‡ç‚¹è€ƒè™‘çš„ä¸¤ä¸ªç»´åº¦ã€‚ æˆ‘ä»¬åŸºäºå¯å‘å¼è§„åˆ™å’Œè´¨é‡æ¨¡å‹æ‰“åˆ†ï¼Œå¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œç¯‡ç« å’Œå¥å­ç²’åº¦çš„è¿‡æ»¤ã€‚åœ¨å…¨é‡æ•°æ®ä¸Šï¼Œåˆ©ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œæ–¹æ³•ï¼Œå¯¹ç¯‡ç« å’Œå¥å­ç²’åº¦åšæ»¤é‡ã€‚\n\næ•´ä½“æµç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼š\n<p align="center">\n <br>\n <img src="media/data_process.png" width="90%"/>\n <br>\n</p>\n\n* ç»è¿‡ä¸æ–­çš„è°ƒæ•´å’Œå¤šè½®æµ‹è¯•ï¼Œæœ€ç»ˆç¡®è®¤äº†ä¸€ä¸ªåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¥½çš„ä¸­è‹±æ–‡é…æ¯”ã€‚\n* æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŸºäºè‡ªåŠ¨å­¦ä¹ çš„æ•°æ®æƒé‡ç­–ç•¥ï¼Œå¯¹ä¸åŒç±»åˆ«çš„æ•°æ®è¿›è¡Œé…æ¯”ã€‚\n\n## åˆ†è¯\næˆ‘ä»¬å‚è€ƒå­¦æœ¯ç•Œæ–¹æ¡ˆä½¿ç”¨ SentencePiece ä¸­çš„ byte pair encoding (BPE)ä½œä¸ºåˆ†è¯ç®—æ³•ï¼Œå¹¶ä¸”è¿›è¡Œäº†ä»¥ä¸‹çš„ä¼˜åŒ–ï¼š\n1. ç›®å‰å¤§éƒ¨åˆ†å¼€æºæ¨¡å‹ä¸»è¦åŸºäºè‹±æ–‡ä¼˜åŒ–ï¼Œå› æ­¤å¯¹ä¸­æ–‡è¯­æ–™å­˜åœ¨æ•ˆç‡è¾ƒä½çš„é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨2000ä¸‡æ¡ä»¥ä¸­è‹±ä¸ºä¸»çš„å¤šè¯­è¨€è¯­æ–™è®­ç»ƒåˆ†è¯æ¨¡å‹ï¼Œæ˜¾è‘—æå‡å¯¹äºä¸­æ–‡çš„å‹ç¼©ç‡ã€‚\n2. å¯¹äºæ•°å­¦é¢†åŸŸï¼Œæˆ‘ä»¬å‚è€ƒäº† LLaMA å’Œ Galactica ä¸­çš„æ–¹æ¡ˆï¼Œå¯¹æ•°å­—çš„æ¯ä¸€ä½å•ç‹¬åˆ†å¼€ï¼Œé¿å…å‡ºç°æ•°å­—ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯¹äºæå‡æ•°å­¦èƒ½åŠ›æœ‰é‡è¦å¸®åŠ©ã€‚\n3. å¯¹äºç½•è§å­—è¯ï¼ˆå¦‚ç‰¹æ®Šç¬¦å·ç­‰ï¼‰ï¼Œæ”¯æŒ UTF-8-characters çš„ byte ç¼–ç ï¼Œå› æ­¤åšåˆ°æœªçŸ¥å­—è¯çš„å…¨è¦†ç›–ã€‚ \n4. æˆ‘ä»¬åˆ†æäº†ä¸åŒåˆ†è¯å™¨å¯¹è¯­æ–™çš„å‹ç¼©ç‡ï¼Œå¦‚ä¸‹è¡¨ï¼Œå¯è§æˆ‘ä»¬çš„åˆ†è¯å™¨æ˜æ˜¾ä¼˜äº LLaMA, Falcon ç­‰å¼€æºæ¨¡å‹ï¼Œå¹¶ä¸”å¯¹æ¯”å…¶ä»–ä¸­æ–‡åˆ†è¯å™¨åœ¨å‹ç¼©ç‡ç›¸å½“çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒå’Œæ¨ç†æ•ˆç‡æ›´é«˜ã€‚\n\n| Model | baichuan-7B | LLaMA | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n|---------------|-------------|-------|--------|--------|---------|---------------|\n| Compress Rate | 0.737 | 1.312 | 1.049 | 1.206 | 0.631 |', metadata={}), Document(page_content='0.659 |\n| Vocab Size | 64000 | 32000 | 65024 | 50254 | 130344 | 106029 |\n\n## æ¨¡å‹ç»“æ„\næ•´ä½“æ¨¡å‹åŸºäºæ ‡å‡†çš„ Transformer ç»“æ„ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å’Œ LLaMA ä¸€æ ·çš„æ¨¡å‹è®¾è®¡\n* ä½ç½®ç¼–ç ï¼š[rotary-embedding](https://arxiv.org/abs/2104.09864) æ˜¯ç°é˜¶æ®µè¢«å¤§å¤šæ¨¡å‹é‡‡ç”¨çš„ä½ç½®ç¼–ç æ–¹æ¡ˆï¼Œå…·æœ‰æ›´å¥½çš„å¤–å»¶æ•ˆæœã€‚è™½ç„¶è®­ç»ƒè¿‡ç¨‹ä¸­æœ€å¤§é•¿åº¦ä¸º4096ï¼Œä½†æ˜¯å®é™…æµ‹è¯•ä¸­æ¨¡å‹å¯ä»¥å¾ˆå¥½çš„æ‰©å±•åˆ° 5000 tokens ä¸Šï¼Œå¦‚ä¸‹å›¾ï¼š\n <p align="center">\n <br>\n <img src="media/long-context-ppl.png" width="90%"/>\n <br>\n </p>\n* æ¿€æ´»å±‚ï¼šSwiGLU, Feedforward å˜åŒ–ä¸º(8/3)å€çš„éšå«å±‚å¤§å°ï¼Œå³11008\n* Layer-Normalization: åŸºäº [RMSNorm](https://arxiv.org/abs/1910.07467) çš„ Pre-Normalization\n\n## è®­ç»ƒç¨³å®šæ€§å’Œåå\næˆ‘ä»¬åœ¨åŸæœ¬çš„LLaMAæ¡†æ¶ä¸Šè¿›è¡Œè¯¸å¤šä¿®æ”¹ä»¥æå‡è®­ç»ƒæ—¶çš„ååï¼Œå…·ä½“åŒ…æ‹¬ï¼š\n1. ç®—å­ä¼˜åŒ–æŠ€æœ¯ï¼šé‡‡ç”¨æ›´é«˜æ•ˆç®—å­ï¼Œå¦‚ Flash-attentionï¼ŒNVIDIA apex çš„ RMSNorm ç­‰ã€‚ \n2. ç®—å­åˆ‡åˆ†æŠ€æœ¯ï¼šå°†éƒ¨åˆ†è®¡ç®—ç®—å­è¿›è¡Œåˆ‡åˆ†ï¼Œå‡å°å†…å­˜å³°å€¼ã€‚ \n3. æ··åˆç²¾åº¦æŠ€æœ¯ï¼šé™ä½åœ¨ä¸æŸå¤±æ¨¡å‹ç²¾åº¦çš„æƒ…å†µä¸‹åŠ é€Ÿè®¡ç®—è¿‡ç¨‹ã€‚ \n4. è®­ç»ƒå®¹ç¾æŠ€æœ¯ï¼šè®­ç»ƒå¹³å°å’Œè®­ç»ƒæ¡†æ¶è”åˆä¼˜åŒ–ï¼ŒIaaS + PaaS å®ç°åˆ†é’Ÿçº§çš„æ•…éšœå®šä½å’Œä»»åŠ¡æ¢å¤ã€‚ \n5. é€šä¿¡ä¼˜åŒ–æŠ€æœ¯ï¼Œå…·ä½“åŒ…æ‹¬ï¼š \n 1. é‡‡ç”¨æ‹“æ‰‘æ„ŸçŸ¥çš„é›†åˆé€šä¿¡ç®—æ³•ï¼Œé¿å…ç½‘ç»œæ‹¥å¡é—®é¢˜ï¼Œæé«˜é€šä¿¡æ•ˆç‡ã€‚ \n 2. æ ¹æ®å¡æ•°è‡ªé€‚åº”è®¾ç½® bucket sizeï¼Œæé«˜å¸¦å®½åˆ©ç”¨ç‡ã€‚ \n 3. æ ¹æ®æ¨¡å‹å’Œé›†ç¾¤ç¯å¢ƒï¼Œè°ƒä¼˜é€šä¿¡åŸè¯­çš„è§¦å‘æ—¶æœºï¼Œä»è€Œå°†è®¡ç®—å’Œé€šä¿¡é‡å ã€‚\n\nåŸºäºä¸Šè¿°çš„å‡ ä¸ªä¼˜åŒ–æŠ€æœ¯ï¼Œæˆ‘ä»¬åœ¨åƒå¡A800æœºå™¨ä¸Šè¾¾åˆ°äº†7Bæ¨¡å‹182Tflopsçš„ååï¼ŒGPUå³°å€¼ç®—åŠ›åˆ©ç”¨ç‡é«˜è¾¾58.3% ã€‚\n \n\næœ€ç»ˆçš„losså¦‚ä¸‹å›¾ï¼š\n<p align="center">\n <br>\n <img src="media/7b.loss.png" width="90%"/>\n <br>\n</p>\n\n#', metadata={}), Document(page_content='å…¬å¼€benchmarkæ¦œå•\n\n## ä¸­æ–‡è¯„æµ‹\n### C-Eval\n[C-Eval æ•°æ®é›†](https://cevalbenchmark.com/index.html)æ˜¯ä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†52ä¸ªå­¦ç§‘å’Œå››ä¸ªéš¾åº¦çš„çº§åˆ«ã€‚æˆ‘ä»¬ä½¿ç”¨è¯¥æ•°æ®é›†çš„devé›†ä½œä¸º few-shot çš„æ¥æºï¼Œåœ¨ test é›†ä¸Šè¿›è¡Œäº† 5-shot æµ‹è¯•ã€‚\n\nå…ˆä¿®æ”¹ `evaluate_zh.py` ä¸­çš„ OPENMODEL_PATH å’Œ CEVAL_DATA_PATH ä¸¤ä¸ªå€¼ï¼Œåˆ†åˆ«æ˜¯æ¨¡å‹ï¼ˆæ–‡ä»¶å¤¹ï¼‰å­˜æ”¾çš„è·¯å¾„å’Œ C-Eval æ•°æ®é›†çš„è·¯å¾„ã€‚å†æ‰§è¡Œä¸‹é¢çš„è„šæœ¬ã€‚\n\n```shell\nshot=5 # few-shot\ngpu=0 # æ˜¾å¡id\nsplit=test # è¯„ä¼°æµ‹è¯•é›†\nmodel_id=baichuan-7b # å¾…è¯„ä¼°çš„æ¨¡å‹\ntask=ceval # ä»»åŠ¡åç§°ï¼šceval\necho gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot\nnohup python evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail > ${model_id}_${task}_${split}_${shot}-shot_record.txt 2>&1 &\n```\n\n### ç»“æœ\n\n| Model 5-shot | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n| Claude-v1.3 | 54.2 | 39.0 | 51.9 | 61.7 | 52.1 | 53.7 |\n|', metadata={}), Document(page_content='Claude-instant-v1.0 | 45.9 | 35.5 | 43.1 | 53.8 | 44.2 | 45.4 |\n| moss-moon-003-base (16B) | 27.4 | 24.5 | 27.0 | 29.1 | 27.2 | 26.9 |\n| Ziya-LLaMA-13B-pretrain | 30.2 | 22.7 | 27.7 | 34.4 | 32.0 | 28.9 |\n| LLaMA-7B-hf | 27.1 | 25.9 | 27.1 | 26.8 | 27.9 | 26.3 |\n| ChatGLM-6B | 34.5 | 23.1 | 30.4 | 39.6 | 37.4 | 34.5 |\n| Falcon-7B | 25.8 | 24.3 | 25.8 | 26.0 | 25.8 | 25.6 |\n| Open-LLaMA-v2-pretrain (7B) | 24.0 | 22.5 | 23.1 | 25.3 | 25.2 | 23.2 |\n| TigerBot-7B-base | 25.7 | 27.0 | 27.3 | 24.7 | 23.4 | 26.1 |\n| Aquila-7B<sup>*</sup> | 25.5 | 25.2 | 25.6 | 24.6 | 25.2 | 26.6 |\n| BLOOM-7B | 22.8 | 20.2 | 21.8 | 23.3 | 23.9 | 23.3 |\n| BLOOMZ-7B | 35.7 | 25.8 | 31.3 | 43.5 | 36.6 | 35.6 |\n| **baichuan-7B** | 42.8 | 31.5 | 38.2 | 52.0 | 46.2 | 39.3 |\n\n\n### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) æ˜¯ä¸€ä¸ªä»¥ä¸­å›½é«˜è€ƒé¢˜ä½œä¸ºè¯„æµ‹å¤§è¯­è¨€æ¨¡å‹èƒ½åŠ›çš„æ•°æ®é›†ï¼Œç”¨ä»¥è¯„ä¼°æ¨¡å‹çš„è¯­è¨€èƒ½åŠ›å’Œé€»è¾‘æ¨ç†èƒ½åŠ›ã€‚\næˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œéšæœºåˆ’åˆ†åå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œç»Ÿä¸€ 5-shot æµ‹è¯•ã€‚\n\n### ç»“æœ\nä»¥ä¸‹æ˜¯æµ‹è¯•çš„ç»“æœã€‚\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 21.41 |\n|', metadata={}), Document(page_content='Ziya-LLaMA-13B-pretrain | 23.17 |\n| Falcon-7B | 23.98 |\n| TigerBot-7B-base | 25.94 |\n| LLaMA-7B | 27.81 |\n| ChatGLM-6B | 21.41 |\n| BLOOM-7B | 26.96 |\n| BLOOMZ-7B | 28.72 |\n| Aquila-7B<sup>*</sup> | 24.39 |\n| **baichuan-7B** | **36.24** |\n\n\n### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) æ—¨åœ¨è¯„ä¼°æ¨¡å‹çš„è®¤çŸ¥å’Œè§£å†³é—®é¢˜ç›¸å…³çš„ä»»åŠ¡ä¸­çš„ä¸€èˆ¬èƒ½åŠ›ã€‚\næˆ‘ä»¬åªä¿ç•™äº†å…¶ä¸­çš„å››é€‰ä¸€å•é¡¹é€‰æ‹©é¢˜ï¼Œéšæœºåˆ’åˆ†åå¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œäº†ç»Ÿä¸€5-shotæµ‹è¯•ã€‚\n\n### ç»“æœ\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 23.49 |\n| Ziya-LLaMA-13B-pretrain | 27.64 |\n| Falcon-7B | 27.18 |\n| TigerBot-7B-base | 25.19 |\n| LLaMA-7B | 28.17 |\n| ChatGLM-6B | 23.49 |\n| BLOOM-7B | 26.55 |\n| BLOOMZ-7B | 30.27 |\n| Aquila-7B<sup>*</sup> | 25.58 |\n| **baichuan-7B** | **34.44** |\n\n<sup>*</sup>å…¶ä¸­ Aquila æ¨¡å‹æ¥æºäºæ™ºæºå®˜æ–¹ç½‘ç«™(https://model.baai.ac.cn/model-detail/100098) ä»…åšå‚è€ƒ\n\n## è‹±æ–‡æ¦œå•\né™¤äº†ä¸­æ–‡ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæµ‹è¯•äº†æ¨¡å‹åœ¨è‹±æ–‡ä¸Šçš„æ•ˆæœï¼Œ[MMLU](https://arxiv.org/abs/2009.03300) æ˜¯åŒ…å«57ä¸ªå¤šé€‰ä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰ï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæ˜¯ç›®å‰ä¸»æµçš„LLMè¯„æµ‹æ•°æ®é›†ã€‚\n\næˆ‘ä»¬é‡‡ç”¨äº†[å¼€æº](https://github.com/hendrycks/test) çš„è¯„æµ‹æ–¹æ¡ˆï¼Œæœ€ç»ˆ 5-shot', metadata={}), Document(page_content='ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š\n\n### ç»“æœ\n\n| Model | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n| LLaMA-7B<sup>2</sup> | 34.0 | 38.3 | 30.5 | 38.1 | 35.1 |\n| Falcon-7B<sup>1</sup> | - | - | - | - | 35.0 |\n| mpt-7B<sup>1</sup> | - | - | - | - | 35.6 |\n| ChatGLM-6B<sup>0</sup> | 35.4 | 41.0 | 31.3 | 40.5 | 36.9 |\n| BLOOM-7B<sup>0</sup> | 25.0 | 24.4 | 26.5 | 26.4 | 25.5 |\n| BLOOMZ-7B<sup>0</sup> | 31.3 | 42.1 | 34.4 | 39.0 | 36.1 |\n| moss-moon-003-base (16B)<sup>0</sup> | 24.2 | 22.8 | 22.4 | 24.4 | 23.6 |\n| moss-moon-003-sft (16B)<sup>0</sup> | 30.5 | 33.8 | 29.3 | 34.4 | 31.9 |\n| **baichuan-7B<sup>0</sup>** | **38.4** | **48.9** | **35.6** | **48.1** | **42.3** |\n\n### ä¸Šæ ‡è¯´æ˜ï¼š\n\n 0:é‡æ–°å¤ç°\n 1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n 2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n\n### å¤ç°æ–¹æ³•\n```shell\ngit clone https://github.com/hendrycks/test\ncd test\nwget', metadata={}), Document(page_content='https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data\nmkdir results\ncp evaluate_mmlu.py .\npython evaluation/evaluate_mmlu.py -m /path/to/baichuan-7b\n\n```\n\nå…¶ä¸­åœ¨ MMLU ä¸Š57ä¸ªä»»åŠ¡çš„å…·ä½“ç»†æŒ‡æ ‡å¦‚ä¸‹å›¾ï¼š\n<p align="center">\n <br>\n <img src="media/MMLU-57-tasks.png" width="90%"/>\n <br>\n</p>\n\nå…¶ä¸­å„ä¸ªå­¦ç§‘çš„æŒ‡æ ‡å¦‚ä¸‹å›¾ï¼š\n<p align="center">\n <br>\n <img src="media/MMLU 21 Subjects.png" width="90%"/>\n <br>\n</p>\n\n# æ¨ç†æ–¹æ³•\n\næ¨ç†ä»£ç å·²ç»åœ¨[å®˜æ–¹ Huggingface åº“](https://huggingface.co/baichuan-inc/baichuan-7B) \n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(\'ç™»é¹³é›€æ¥¼->ç‹ä¹‹æ¶£\\nå¤œé›¨å¯„åŒ—->\', return_tensors=\'pt\')\ninputs = inputs.to(\'cuda:0\')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n# è®­ç»ƒæ–¹æ³•\n## å®‰è£…ä¾èµ–\n```shell\npip install -r', metadata={}), Document(page_content='requirements.txt\n```\n## å‡†å¤‡æ•°æ®\nç”¨æˆ·å°†è®­ç»ƒè¯­æ–™æŒ‰æ€»rankæ•°çš„å€æ•°å‡åŒ€åˆ‡åˆ†æˆå¤šä¸ª UTF-8 æ–‡æœ¬æ–‡ä»¶ï¼Œæ”¾ç½®åœ¨è¯­æ–™ç›®å½•ï¼ˆé»˜è®¤ä¸º `data_dir` ï¼‰ä¸‹ã€‚å„ä¸ªrankè¿›ç¨‹å°†ä¼šè¯»å–è¯­æ–™ç›®å½•ä¸‹çš„ä¸åŒæ–‡ä»¶ï¼Œå…¨éƒ¨åŠ è½½åˆ°å†…å­˜åï¼Œå¼€å§‹åç»­è®­ç»ƒè¿‡ç¨‹ã€‚ä»¥ä¸Šæ˜¯ç®€åŒ–çš„ç¤ºèŒƒæµç¨‹ï¼Œå»ºè®®ç”¨æˆ·åœ¨æ­£å¼è®­ç»ƒä»»åŠ¡ä¸­ï¼Œæ ¹æ®éœ€æ±‚è°ƒæ•´æ•°æ®ç”Ÿäº§é€»è¾‘ã€‚\n\n## ä¸‹è½½ tokenizer æ¨¡å‹\nä¸‹è½½ tokenizer æ¨¡å‹æ–‡ä»¶ [tokenizer.model](https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model) ï¼Œæ”¾ç½®åœ¨é¡¹ç›®ç›®å½•ä¸‹ã€‚\n \n## é…ç½® DeepSpeed\næœ¬ç¤ºèŒƒä»£ç é‡‡ç”¨ DeepSpeed æ¡†æ¶è¿›è¡Œè®­ç»ƒã€‚ç”¨æˆ·éœ€æ ¹æ®é›†ç¾¤æƒ…å†µï¼Œä¿®æ”¹ `config/hostfile` ï¼Œå¦‚æœæ˜¯å¤šæœºå¤šå¡ï¼Œéœ€è¦ä¿®æ”¹ ssh ä¸­å„ä¸ªèŠ‚ç‚¹çš„ IP é…ç½®ã€‚å…·ä½“å¯ä»¥å‚è§DeepSpeed[å®˜æ–¹è¯´æ˜](https://www.deepspeed.ai/) ã€‚\n\n## æ‰§è¡Œè®­ç»ƒ\n```python\nscripts/train.sh\n```\n\n# åè®®\nå¯¹æœ¬ä»“åº“æºç çš„ä½¿ç”¨éµå¾ªå¼€æºè®¸å¯åè®® [Apache 2.0](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)ã€‚\n\nbaichuan-7Bæ”¯æŒå•†ç”¨ã€‚å¦‚æœå°†baichuan-7B æ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨ä½œå•†ä¸šç”¨é€”ï¼Œè¯·æ‚¨æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è”ç³»è®¸å¯æ–¹ï¼Œä»¥è¿›è¡Œç™»è®°å¹¶å‘è®¸å¯æ–¹ç”³è¯·ä¹¦é¢æˆæƒï¼šè”ç³»é‚®ç®±ï¼šopensource@baichuan-inc.comï¼Œ å…·ä½“è®¸å¯åè®®å¯è§[ã€Šbaichuan-7B æ¨¡å‹è®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)ã€‚', metadata={}), Document(page_content='<div align="center">\n <h1> baichuan-7B </h1>\n<p align="center" style="display: flex; flex-direction: row; justify-content: center; align-items: center">\n ğŸ¤— \n <a href="https://huggingface.co/baichuan-inc/baichuan-7B" target="_blank" style="margin-right: 15px; margin-left: 10px">Hugging Face</a> â€¢ \n ğŸ¤–\n <a href="https://modelscope.cn/organization/baichuan-inc" target="_blank" style="margin-left: 10px">ModelScope</a > â€¢\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/media/wechat.jpeg?raw=true" target="_blank" rel="noopener noreferrer" style="display: inline-block; margin-left: 10px">\n <span style="color: blue;">Wechat</span>\n </a>\n </p>\n\n\n[![license](https://img.shields.io/github/license/modelscope/modelscope.svg)](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE)\n<h4 align="center">\n <p>\n <b>English</b> |\n <a href="https://github.com/baichuan-inc/baichuan-7B/blob/main/README.md">ä¸­æ–‡</a>\n <p>\n</h4>\n\n\n</div>\n\n# Introduction\n\nbaichuan-7B is an open-source, large-scale pre-training', metadata={}), Document(page_content='language model developed by Baichuan Intelligent Technology. baichuan-7B is based on Transformer architecture, which contains 7 billion parameter and trained on approximately 1.2 trillion tokens. It supports both Chinese and English languages with a context window length of 4096. It has achieved the best performance among models of the same size on standard Chinese and English authoritative benchmarks (C-EVAL, MMLU, etc).\n\n## Data\n\n* The original corpora includes open-source Chinese and English data, self-crawled Chinese internet data, and some high-quality knowledge-intensive data.\n* Referring to related data work, frequency and quality are two dimensions that are considered important in the data processing stage. We apply heuristic rules and quality model scoring to filter the original dataset at both the paragraph and sentence levels. Employing the Locality-Sensitive Hashing (LSH) method on the full dataset, we perform de-duplication at both the paragraph and sentence levels.\n\nThe whole data processing', metadata={}), Document(page_content='process is shown below:\n<p align="center">\n <br>\n <img src="media/data_process.png" width="90%"/>\n <br>\n</p>\n\n* After continuous adjustments and multiple rounds of testing, we finally determined the best Chinese to English ratio that are optimized on downstream tasks.\n* We used an automatic algorithm-based data sampling strategy to balance the weights of different data categories.\n\n## Tokenization\nWe use the byte pair encoding (BPE) from SentencePiece as the tokenization algorithm, along with the following optimizations:\n\n1. Most open-source models are primarily optimized for English, resulting in low efficiency for Chinese corpus. So we trained the tokenizer using 20 million multilingual corpora mainly composed of Chinese and English, significantly improving the compression rate for Chinese.\n2. To improve the ability for mathmatics, we split all numbers into individual digits that is also adopted in LLaMA and Galactica, separately tokenizing each digit to avoid inconsistencies in numbers.\n3. For rare words', metadata={}), Document(page_content='(such as emoji and special symbols), we fallback unknown characters to byte encoding of UTF-8, thus achieving full coverage of unknown words.\n4. We analyzed the compression rate of different tokenizers on the corpus. As shown in the following table, our tokenizer significantly outperforms open-source models like LLaMA, Falcon, and others. Compared to other Chinese tokenizers with similar compression rates, it offers higher training and inference efficiency. \n\n| Model | baichuan-7B | LLaMA | Falcon | mpt-7B | ChatGLM | moss-moon-003 |\n|---------------|-------------|-------|--------|--------|---------|---------------|\n| Compress Rate | 0.737 | 1.312 | 1.049 | 1.206 | 0.631 | 0.659 |\n| Vocab Size | 64000 | 32000 | 65024 | 50254 | 130344 | 106029 |\n\n## Model Architecture\nThe overall model is based on the standard Transformer structure, and we have adopted a model design similar to that of LLaMA.\n* Positional Embeddings: [rotary-embedding](https://arxiv.org/abs/2104.09864) is the widely used positional encoding', metadata={}), Document(page_content='method, with better extrapolation effects. Although the maximum length during training is 4096, the model can be well extrapolated to 5000 tokens in inference time, as shown in the following diagram:\n <p align="center">\n <br>\n <img src="media/long-context-ppl.png" width="90%"/>\n <br>\n </p>\n* Activationï¼šSwiGLU, and the dimension of the feedforward-layer is set to 11008\n* Layer-Normalization: We use the Pre-Normalization method based on [RMSNorm](https://arxiv.org/abs/1910.07467)\n\n## Training stability and Throughput\nWe made numerous modifications to the original LLaMA framework to improve throughput during training, including:\n\n1. Operator optimization technology: We adopted more efficient operators, such as Flash-attention, NVIDIA apex\'s RMSNorm, etc.\n2. Tensor partitioning technology: We partitioned some computational operators to reduce peak memory usage.\n3. Mixed-precision technology: This accelerates the computational process without sacrificing model accuracy.\n4. Training failure recovery technology: The', metadata={}), Document(page_content='training platform and the training framework were jointly optimized. By combining IaaS and PaaS, we can locate faults and recover tasks within minutes.\n5. Communication optimization technology which includes:\n 1. Topology-aware collective communication algorithms to avoid network congestion and improve communication efficiency.\n 2. Adaptive setting of bucket size based on the number of cards to improve bandwidth utilization.\n 3. Tuning the trigger timing of communication primitives based on the model and the cluster environment, thereby overlapping computation and communication.\n \nBy using these optimization techniques, we achieved a throughput of 182 Tflops for the 7B model on thousand A800 GPUs, with a peak GPU computing power utilization rate of up to 58.3%.\n\nThe final loss of the model is shown belowï¼š\n<p align="center">\n <br>\n <img src="media/7b.loss.png" width="90%"/>\n <br>\n</p>\n\n# Benchmark\n\n## Chinese Benchmarks\n### C-Eval\n[C-Eval](https://cevalbenchmark.com/index.html) is a comprehensive Chinese', metadata={}), Document(page_content='language models evaluation dataset, covering 52 subjects and four levels of difficulty. We used the dev set from this dataset as the source for few-shot learning and conducted a 5-shot test on the test set.\n\n\nChange OPENMODEL_PATH and CEVAL_DATA_PATH in evaluate_zh.py, corresponding to model directory and C-Eval dataset, and runing:\n```shell\nshot=5 # few-shot\ngpu=0 # GPUid\nsplit=test # test set\nmodel_id=baichuan-7b # model\ntask=ceval # task nameï¼šceval\necho gpu_idx-${gpu}-${model_id}_${task}_${split}_${shot}-shot\nnohup python evaluate_zh.py --gpu_idx ${gpu} --model_id ${model_id} --task ${task} --shot ${shot} --split ${split} --show_detail > ${model_id}_${task}_${split}_${shot}-shot_record.txt 2>&1 &\n\n```\n\n### Result\n\n\n| Model 5-shot | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4 | 68.7 | 54.9 | 67.1 | 77.6 | 64.5 | 67.8 |\n| ChatGPT | 54.4 | 41.4 | 52.9 | 61.8 | 50.9 | 53.6 |\n|', metadata={}), Document(page_content='Claude-v1.3 | 54.2 | 39.0 | 51.9 | 61.7 | 52.1 | 53.7 |\n| Claude-instant-v1.0 | 45.9 | 35.5 | 43.1 | 53.8 | 44.2 | 45.4 |\n| moss-moon-003-base (16B) | 27.4 | 24.5 | 27.0 | 29.1 | 27.2 | 26.9 |\n| Ziya-LLaMA-13B-pretrain | 30.2 | 22.7 | 27.7 | 34.4 | 32.0 | 28.9 |\n| LLaMA-7B-hf | 27.1 | 25.9 | 27.1 | 26.8 | 27.9 | 26.3 |\n| ChatGLM-6B | 34.5 | 23.1 | 30.4 | 39.6 | 37.4 | 34.5 |\n| Falcon-7B | 25.8 | 24.3 | 25.8 | 26.0 | 25.8 | 25.6 |\n| Open-LLaMA-v2-pretrain (7B) | 24.0 | 22.5 | 23.1 | 25.3 | 25.2 | 23.2 |\n| TigerBot-7B-base | 25.7 | 27.0 | 27.3 | 24.7 | 23.4 | 26.1 |\n| Aquila-7B<sup>*</sup> | 25.5 | 25.2 | 25.6 | 24.6 | 25.2 | 26.6 |\n| BLOOM-7B | 22.8 | 20.2 | 21.8 | 23.3 | 23.9 | 23.3 |\n| BLOOMZ-7B | 35.7 | 25.8 | 31.3 | 43.5 | 36.6 | 35.6 |\n| **baichuan-7B** | 42.8 | 31.5 | 38.2 | 52.0 | 46.2 | 39.3 |\n\n\n### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) is an evaluation dataset used in Chinese college entrance examination questions to evaluate the capabilities of large language models, assessing the', metadata={}), Document(page_content="model's language ability and logical reasoning skills. We processed the dataset to only containing the single-answer multiple choice questions, we conducted a 5-shot test on all models.\n\n### Results\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 21.41 |\n| Ziya-LLaMA-13B-pretrain | 23.17 |\n| Falcon-7B | 23.98 |\n| TigerBot-7B-base | 25.94 |\n| LLaMA-7B | 27.81 |\n| ChatGLM-6B | 21.41 |\n| BLOOM-7B | 26.96 |\n| BLOOMZ-7B | 28.72 |\n| Aquila-7B<sup>*</sup> | 24.39 |\n| **baichuan-7B** | **36.24** |\n\n\n### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) is a dataset aimed at evaluating the model's general abilities in cognitive and problem-solving tasks.\nwe conducted a 5-shot test on all models.\n\n### Result\n\n| Model | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain | 23.49 |\n| Ziya-LLaMA-13B-pretrain | 27.64 |\n| Falcon-7B | 27.18 |\n| TigerBot-7B-base | 25.19 |\n| LLaMA-7B | 28.17 |\n| ChatGLM-6B | 23.49 |\n| BLOOM-7B | 26.55 |\n|", metadata={}), Document(page_content='BLOOMZ-7B | 30.27 |\n| Aquila-7B<sup>*</sup> | 25.58 |\n| **baichuan-7B** | **34.44** |\n\n<sup>*</sup>The Aquila-7b are not implemented on Huggingface yet so we derived the model from (https://model.baai.ac.cn/model-detail/100098), which may have not identical to their official result.\n\n## English Benchmarks\nIn addition to Chinese, we also tested the performance of the model in English. [MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty spans from high school level to expert level, making it a mainstream evaluation dataset for Large Language Models (LLMs).\n\nWe adopt the public implementation of (https://github.com/hendrycks/test) and the final result is shwon belowï¼š\n\n### Results on MMLU\n\n| Model | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n|', metadata={}), Document(page_content='LLaMA-7B<sup>2</sup> | 34.0 | 38.3 | 30.5 | 38.1 | 35.1 |\n| Falcon-7B<sup>1</sup> | - | - | - | - | 35.0 |\n| mpt-7B<sup>1</sup> | - | - | - | - | 35.6 |\n| ChatGLM-6B<sup>0</sup> | 35.4 | 41.0 | 31.3 | 40.5 | 36.9 |\n| BLOOM 7B<sup>0</sup> | 25.0 | 24.4 | 26.5 | 26.4 | 25.5 |\n| BLOOMZ 7B<sup>0</sup> | 31.3 | 42.1 | 34.4 | 39.0 | 36.1 |\n| moss-moon-003-base (16B)<sup>0</sup> | 24.2 | 22.8 | 22.4 | 24.4 | 23.6 |\n| moss-moon-003-sft (16B)<sup>0</sup> | 30.5 | 33.8 | 29.3 | 34.4 | 31.9 |\n| **baichuan-7B<sup>0</sup>** | **38.4** | **48.9** | **35.6** | **48.1** | **42.3** |\n\n### Notesï¼š\n\n 0:Our implementation\n 1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n 2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n\n### How to implement by yourself\n```shell\ngit clone https://github.com/hendrycks/test\ncd test\nwget https://people.eecs.berkeley.edu/~hendrycks/data.tar\ntar xf data\nmkdir results\ncp evaluate_mmlu.py .\npython evaluation/evaluate_mmlu.py -m', metadata={}), Document(page_content='/path/to/baichuan-7b\n\n```\n\nSpecifically, the result of 57 MMLU tasks isï¼š\n<p align="center">\n <br>\n <img src="media/MMLU-57-tasks.png" width="90%"/>\n <br>\n</p>\n\nAnd the comparison of 21 different subjects isï¼š\n<p align="center">\n <br>\n <img src="media/MMLU 21 Subjects.png" width="90%"/>\n <br>\n</p>\n\n# Inference\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(\'Hamlet->Shakespeare\\nOne Hundred Years of Solitude->\', return_tensors=\'pt\')\ninputs = inputs.to(\'cuda:0\')\npred = model.generate(**inputs, max_new_tokens=64)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n\n```\n\n# Training\n## Install requirements\n```shell\npip install -r requirements.txt\n```\n## Prepare pre-training datasets\nYou should divide the training corpus into multiple UTF-8 text files', metadata={}), Document(page_content='evenly according to the multiple of the total rank number, and place them in the corpus directory (default is `data_dir`). Each rank processor will read different files in the corpus directory, load them all into memory, and then start the subsequent training process. The above is a simplified demonstration process. It is recommended that users adjust the data production logic according to their needs in formal training tasks.\n\n## Download tokenizer\nYou can download our [tokenizer.model](https://huggingface.co/baichuan-inc/baichuan-7B/blob/main/tokenizer.model) from the Huggingface, and place them in the root director.\n \n## Config DeepSpeed\nThis demo code uses the DeepSpeed framework for training. Users should modify `config/hostfile` according to the cluster conditions.\n\n## Start training\n```shell\nscripts/train.sh\n```\n\n\n# Licences\nThe use of the source code in this repository is governed by the open source license [Apache 2.0](https://github.com/baichuan-inc/baichuan-7B/blob/main/LICENSE) .\n\nThe use of the', metadata={}), Document(page_content='baichuan-7B model weights, however, must follow the [ã€Šbaichuan-7B æ¨¡å‹è®¸å¯åè®®ã€‹](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) .', metadata={})]